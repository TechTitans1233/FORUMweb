
from flask import Flask, request, jsonify
from flask_cors import CORS
import joblib
import glob
import os
import json
import hashlib
import pandas as pd
import datetime
import random
from geopy.geocoders import Nominatim
from geopy.distance import geodesic
import unicodedata
from turfpy.measurement import area
from geojson import Feature

app = Flask(__name__)
CORS(app)

# --- Configura√ß√µes de Arquivos ---
REAL_POSTS_FILE = 'real_posts.json'
DATASET_REAL_COMBINED_PATH = 'dataset_real_coletado.csv'
JSON_CITIES_FILE = "brazil_states_cities_geocoded.json"

# --- Vari√°veis Globais para Publica√ß√µes Reais ---
loaded_posts_set = set()
loaded_posts_list = []

# --- Configura√ß√£o do Geocodificador ---
geolocator = Nominatim(user_agent="xai_environmental_api")

# --- Dados de Localiza√ß√£o e Impacto ---
locations_data = []
impact_data = {
    "baixo impacto a pequena √°rea": {
        "cor": "green",
        "area_km2_min": 1,
        "area_km2_max": 50,
        "sinonimos_intensidade": ["pequeno", "leve", "incipiente", "reduzido", "m√≠nimo", "localizado", "controlado"]
    },
    "baixo impacto a m√©dia √°rea": {
        "cor": "green",
        "area_km2_min": 51,
        "area_km2_max": 200,
        "sinonimos_intensidade": ["pequeno", "leve", "incipiente", "reduzido", "m√≠nimo", "localizado", "controlado"]
    },
    "baixo impacto a grande √°rea": {
        "cor": "green",
        "area_km2_min": 201,
        "area_km2_max": 5000,
        "sinonimos_intensidade": ["pequeno", "leve", "incipiente", "reduzido", "m√≠nimo", "localizado", "controlado"]
    },
    "moderado impacto a pequena √°rea": {
        "cor": "orange",
        "area_km2_min": 1,
        "area_km2_max": 50,
        "sinonimos_intensidade": ["moderado", "consider√°vel", "significativo", "m√©dio", "regular", "importante", "parcial"]
    },
    "moderado impacto a m√©dia √°rea": {
        "cor": "orange",
        "area_km2_min": 51,
        "area_km2_max": 200,
        "sinonimos_intensidade": ["moderado", "consider√°vel", "significativo", "m√©dio", "regular", "importante", "parcial"]
    },
    "moderado impacto a grande √°rea": {
        "cor": "orange",
        "area_km2_min": 201,
        "area_km2_max": 5000,
        "sinonimos_intensidade": ["moderado", "consider√°vel", "significativo", "m√©dio", "regular", "importante", "parcial"]
    },
    "alto impacto a pequena √°rea": {
        "cor": "red",
        "area_km2_min": 1,
        "area_km2_max": 50,
        "sinonimos_intensidade": ["grande", "severo", "extremo", "grave", "intenso", "devastador", "cr√≠tico", "total", "generalizado"]
    },
    "alto impacto a m√©dia √°rea": {
        "cor": "red",
        "area_km2_min": 51,
        "area_km2_max": 200,
        "sinonimos_intensidade": ["grande", "severo", "extremo", "grave", "intenso", "devastador", "cr√≠tico", "total", "generalizado"]
    },
    "alto impacto a grande √°rea": {
        "cor": "red",
        "area_km2_min": 201,
        "area_km2_max": 5000,
        "sinonimos_intensidade": ["grande", "severo", "extremo", "grave", "intenso", "devastador", "cr√≠tico", "total", "generalizado"]
    },
    "indefinido": {
        "cor": "gray",
        "area_km2_min": 0,
        "area_km2_max": 0,
        "sinonimos_intensidade": ["indefinido", "desconhecido", "incerto"]
    }
}

# --- Sin√¥nimos para Valida√ß√£o de Categoria ---
disaster_synonyms_validation = {
    "seca": [
        "estiagem prolongada", "falta de chuva", "crise de √°gua", "aridez extrema",
        "solo seco", "per√≠odo √°rido", "escassez de √°gua", "d√©ficit pluviom√©trico",
        "terra √°rida", "clima seco", "estiagem severa", "falta de umidade",
        "seca brava", "estiagem sem fim", "terra rachada de vez", "sem gota d'√°gua",
        "seca desgra√ßada", "rio seco", "planta morrendo", "choveu nada",
        "reservat√≥rio vazio", "situa√ß√£o h√≠drica cr√≠tica", "√°gua sumindo", "seca de lascar",
        "terras √°ridas", "ver√£o muito seco"
    ],
    "ciclone": [
        "tempestade cicl√¥nica", "furac√£o tropical", "tuf√£o violento", "ciclone mar√≠timo",
        "ventos intensos", "tormenta", "ciclone costeiro", "tempestade", "ventania",
        "ciclone extremo", "sistema cicl√¥nico", "ventos devastadores",
        "ventania absurda", "ciclone pesado", "vento que varre", "furac√£o da pesada",
        "vento louco", "vendaval terr√≠vel", "ciclone doido", "destrui√ß√£o pelo vento",
        "rajadas de vento fort√≠ssimas", "vento de arrancar", "tempestade de vento",
        "vortex gigante", "ciclone assustador"
    ],
    "terremoto": [
        "abalo terrestre", "sismo forte", "tremor s√≠smico", "movimento s√≠smico",
        "terremoto violento", "sismo devastador", "tremor intenso", "choque s√≠smico",
        "atividade tel√∫rica", "sismo moderado", "ruptura do solo",
        "ch√£o balan√ßando muito", "terra tremeu forte", "sacudida geral", "abalo gigante",
        "tremedeira forte", "casa tremendo", "paredes rachando", "sismo violento",
        "tremor no ch√£o", "abalo s√≠smico grave", "terra treme", "solo sacudindo",
        "sismo profundo", "vibra√ß√£o terrestre"
    ],
    "desastre_hidrico": [
        "inunda√ß√£o urbana", "cheia de rio", "alagamento severo", "enchente forte",
        "transbordo de c√≥rrego", "inunda√ß√£o costeira", "chuva torrencial", "mar√© de tempestade",
        "alagamento repentino", "enxurrada forte", "rio transborda", "inunda√ß√£o grave",
        "rua submersa", "√°gua at√© o pesco√ßo", "alagou tudo", "rio passando do limite",
        "cidade debaixo d'√°gua", "alagamento monstro", "chuva sem fim", "barragem estourou",
        "dil√∫vio na cidade", "√°gua subindo r√°pido", "bairro alagado", "corredeira na rua",
        "n√≠vel da √°gua alto", "rios cheios", "situa√ß√£o de alagamento", "chuva", "chuvas",
        "alagamento", "inunda√ß√£o", "enchente"
    ],
    "queimada": [
        "fogo florestal", "inc√™ndio em floresta", "queima descontrolada", "fogo em vegeta√ß√£o",
        "inc√™ndio de mata", "chamas intensas", "fogo rural", "queimada de grandes propor√ß√µes",
        "inc√™ndio em √°rea verde", "fuma√ßa densa", "destrui√ß√£o florestal",
        "inc√™ndio fora de controle", "fogo com for√ßa", "mato em chamas", "floresta virando cinza",
        "fogo incontrol√°vel", "inc√™ndio gigante", "cheiro de fuma√ßa", "cinzas voando",
        "chamas consumindo", "mata em chamas", "inc√™ndio em lavoura", "fogo desgovernado",
        "inc√™ndio devastador", "fuma√ßa t√≥xica"
    ],
    "vulcao": [
        "erup√ß√£o de lava", "vulc√£o em atividade", "explos√£o de cinzas", "atividade vulc√¢nica intensa",
        "vulc√£o explosivo", "fluxo de lava", "erup√ß√£o violenta", "cinzas vulc√¢nicas densas",
        "vulc√£o em erup√ß√£o", "montanha vulc√¢nica", "gases de erup√ß√£o",
        "vulc√£o cuspindo lava", "montanha explodindo", "muita fuma√ßa do vulc√£o", "vulc√£o em f√∫ria",
        "lava escorrendo", "cinzas caindo", "vulc√£o ativo", "barulho do vulc√£o",
        "montanha de fogo despertando", "fluxo pirocl√°stico avan√ßa", "vulc√£o soltando rocha",
        "cinzas vulc√¢nicas", "erup√ß√£o de propor√ß√µes"
    ],
    "deslizamento": [
        "desmoronamento de encosta", "queda de terra", "deslizamento de solo", "soterramento de √°rea",
        "movimento de terra", "desabamento de barreira", "eros√£o de encosta", "deslizamento grave",
        "instabilidade de solo", "desmoronamento rochoso", "fluxo de lama",
        "terra desabando", "morro caindo", "barranco veio abaixo", "lama pra todo lado",
        "encosta caindo", "pedra desprendendo", "casa soterrada", "terreno cedendo",
        "terra deslizando", "desmoronamento de rochas", "lama escorrendo", "barranco ruindo",
        "encosta perigosa", "risco de movimento de massa"
    ],
    "tempestade": [
        "chuva violenta", "temporal forte", "vendaval intenso", "trovoada severa",
        "tempestade com raios", "chuva de granizo forte", "ventania severa", "tormenta el√©trica",
        "tempestade intensa", "ventos fortes", "chuva pesada",
        "temporal de rachar", "chuva que n√£o para", "vendaval assustador", "muita trovoada",
        "chuva de balde", "vento destruidor", "raios e trov√µes", "c√©u escuro",
        "chuva torrencial", "vento que derruba √°rvores", "trov√µes e rel√¢mpagos", "clima adverso",
        "granizo grande", "rajada de vento forte", "chuva", "chuvas"
    ],
    "nao_classificado": [
        "problema n√£o identificado", "alerta gen√©rico", "situa√ß√£o estranha",
        "incidente sem detalhes", "ocorr√™ncia indefinida", "algo aconteceu",
        "n√£o sei o que √©", "confus√£o", "perigo geral", "alerta de algo",
        "situa√ß√£o bizarra", "alerta vago", "coisa doida", "problema de dif√≠cil identifica√ß√£o",
        "sem informa√ß√µes precisas", "desconhecido na √°rea", "incidente sem categoria",
        "situa√ß√£o incomum", "alerta n√£o especificado", "evento sem descri√ß√£o",
        "fen√¥meno estranho", "coisa sem nexo", "informa√ß√£o ruim"
    ]
}

# --- Categorias Improv√°veis por Pa√≠s ---
implausible_categories = {
    "vulcao": ["Brasil"]  # Vulc√µes s√£o raros no Brasil
}

# === Fun√ß√µes de Carregamento ===
def load_brazilian_cities_from_json(json_file_path):
    cities_data_local = []
    try:
        with open(json_file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        brazil_data = data.get("Brazil", {})
        continent = brazil_data.get("continent", "Am√©rica do Sul")
        country = "Brasil"
        for state_name, state_info in brazil_data.get("states", {}).items():
            state_cities = state_info.get("cities", {})
            for city_name, city_info in state_cities.items():
                lat_str = city_info.get("lat")
                lon_str = city_info.get("lon")
                final_lat = None
                final_lon = None
                if lat_str and lon_str:
                    try:
                        final_lat = float(lat_str)
                        final_lon = float(lon_str)
                    except ValueError:
                        pass
                cities_data_local.append({
                    "cidade": city_name,
                    "estado": state_name,
                    "latitude": final_lat,
                    "longitude": final_lon,
                    "continente": continent,
                    "pais": country
                })
        print(f"‚úÖ Carregadas {len(cities_data_local)} localidades do arquivo '{json_file_path}'.")
        return cities_data_local
    except FileNotFoundError:
        print(f"‚ùå Erro: O arquivo '{json_file_path}' n√£o foi encontrado. Usando lista reduzida de locais de exemplo como fallback.")
        return [
            {"continente": "Am√©rica do Sul", "pais": "Brasil", "estado": "S√£o Paulo", "cidade": "S√£o Paulo", "latitude": -23.5505, "longitude": -46.6333},
            {"continente": "Am√©rica do Sul", "pais": "Brasil", "estado": "Rio de Janeiro", "cidade": "Rio de Janeiro", "latitude": -22.9068, "longitude": -43.1729},
            {"continente": "Am√©rica do Sul", "pais": "Brasil", "estado": "Minas Gerais", "cidade": "Belo Horizonte", "latitude": -19.9167, "longitude": -43.9344},
            {"continente": "Am√©rica do Sul", "pais": "Brasil", "estado": "Pernambuco", "cidade": "Garanhuns", "latitude": -8.8803, "longitude": -36.4795}
        ]
    except json.JSONDecodeError:
        print(f"‚ùå Erro: N√£o foi poss√≠vel decodificar o arquivo JSON '{json_file_path}'. Verifique se o formato est√° correto.")
        return []
    except Exception as e:
        print(f"‚ùå Ocorreu um erro inesperado ao carregar o JSON: {e}")
        return []

locations_data = load_brazilian_cities_from_json(JSON_CITIES_FILE)

def carregar_modelo_mais_recente(prefixo):
    base_dir = os.path.dirname(os.path.abspath(__file__))
    nome_simples = os.path.join(base_dir, f"{prefixo}.pkl")
    try:
        if os.path.isfile(nome_simples):
            print(f"üîÅ Carregando modelo (sem timestamp): {nome_simples}")
            return joblib.load(nome_simples)
        arquivos = glob.glob(os.path.join(base_dir, f"{prefixo}_*.pkl"))
        if not arquivos:
            print(f"‚ö†Ô∏è Nenhum arquivo encontrado para o prefixo: {prefixo} no diret√≥rio {base_dir}")
            return None
        arquivos.sort()
        print(f"üîÅ Carregando modelo (com timestamp): {arquivos[-1]}")
        return joblib.load(arquivos[-1])
    except Exception as e:
        print(f"‚ùå Erro ao carregar modelo {prefixo}: {e}")
        return None

modelo_categoria = carregar_modelo_mais_recente("modelo_evento_final")
modelo_localizacao = carregar_modelo_mais_recente("modelo_local_final")
modelo_impacto = carregar_modelo_mais_recente("modelo_impacto_final")

# --- Fun√ß√µes para Gerenciar Publica√ß√µes Reais ---
def load_real_posts():
    global loaded_posts_set, loaded_posts_list
    if os.path.exists(REAL_POSTS_FILE):
        with open(REAL_POSTS_FILE, 'r', encoding='utf-8') as f:
            try:
                data = json.load(f)
                loaded_posts_list = data
                for post in loaded_posts_list:
                    if 'id_hash' in post:
                        loaded_posts_set.add(post['id_hash'])
                    else:
                        message_hash = hashlib.sha256(
                            f"{post.get('titulo', '')}{post.get('conteudo', '')}".encode('utf-8')
                        ).hexdigest()
                        loaded_posts_set.add(message_hash)
                        post['id_hash'] = message_hash
            except json.JSONDecodeError:
                print(f"‚ö†Ô∏è Aviso: Arquivo {REAL_POSTS_FILE} corrompido ou vazio. Iniciando com lista vazia.")
                loaded_posts_set = set()
                loaded_posts_list = []
    print(f"‚úÖ Carregadas {len(loaded_posts_list)} publica√ß√µes existentes para deduplica√ß√£o.")

def save_real_posts():
    with open(REAL_POSTS_FILE, 'w', encoding='utf-8') as f:
        json.dump(loaded_posts_list, f, ensure_ascii=False, indent=4)
    print(f"üíæ Salvas {len(loaded_posts_list)} publica√ß√µes em {REAL_POSTS_FILE}.")

load_real_posts()

# === Fun√ß√£o Auxiliar para Correspond√™ncia de Coordenadas ===
def find_closest_city(lat, lon, max_distance_km=100):
    try:
        lat = round(float(lat), 4)
        lon = round(float(lon), 4)
        closest_city = None
        min_distance = float('inf')
        for loc in locations_data:
            if loc["latitude"] is not None and loc["longitude"] is not None:
                distance = ((lat - loc["latitude"]) ** 2 + (lon - loc["longitude"]) ** 2) ** 0.5 * 111
                if distance < min_distance:
                    min_distance = distance
                    closest_city = loc["cidade"]
        if closest_city and min_distance <= max_distance_km:
            return closest_city
        return None
    except (ValueError, TypeError):
        return None

# === Fun√ß√£o Auxiliar para Geocodifica√ß√£o ===
def get_city_from_coordinates(lat, lon):
    try:
        location = geolocator.reverse((lat, lon), language='pt')
        if location and location.raw.get('address'):
            city = location.raw['address'].get('city') or location.raw['address'].get('town') or location.raw['address'].get('village')
            return city if city else None
        return None
    except Exception as e:
        print(f"‚ùå Erro ao geocodificar coordenadas ({lat}, {lon}): {e}")
        return None

# === Fun√ß√£o Auxiliar para Determinar N√≠vel de Impacto com √Årea ===
def determine_impact_level_with_area(pred_imp, area_km2):
    try:
        area_km2 = float(area_km2)
        if area_km2 > 200:
            area_category = "grande √°rea"
        elif area_km2 > 50:
            area_category = "m√©dia √°rea"
        else:
            area_category = "pequena √°rea"
        base_impact = pred_imp if pred_imp in ["baixo", "moderado", "alto"] else "indefinido"
        impact_key = f"{base_impact} impacto a {area_category}" if base_impact != "indefinido" else "indefinido"
        return impact_key if impact_key in impact_data else "indefinido"
    except (ValueError, TypeError):
        return "indefinido"

# === Fun√ß√£o Auxiliar para Normalizar Nomes de Cidades e Textos ===
def normalize_text(text):
    if not text:
        return ""
    text = unicodedata.normalize('NFKD', text).encode('ASCII', 'ignore').decode('ASCII')
    return text.lower().strip()

# === Fun√ß√µes de Valida√ß√£o ===
def validate_text_quality(text):
    MIN_LENGTH = 50  # Aumentado para exigir mais conte√∫do
    MIN_WORDS = 5    # Exige pelo menos 5 palavras
    IRRELEVANT_WORDS = ["ok", "teste", "n/a", "nada", "aa", "bb", "teste 4k+"]
    if len(text) < MIN_LENGTH:
        return False, "texto muito curto"
    words = text.split()
    if len(words) < MIN_WORDS:
        return False, "poucas palavras"
    if any(word.lower() in IRRELEVANT_WORDS for word in words):
        return False, "cont√©m palavras irrelevantes"
    # Verifica se o texto √© apenas repeti√ß√£o de caracteres ou nonsense
    if len(set(words)) < 2 or all(len(word) <= 2 for word in words):
        return False, "texto sem sentido ou repetitivo"
    return True, "v√°lido"

def validate_timestamp(timestamp, max_days=30):
    try:
        post_date = datetime.datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
        return (datetime.datetime.now() - post_date).days <= max_days, "v√°lido"
    except:
        return False, "timestamp inv√°lido"

def validate_category(text, predicted_category):
    text = normalize_text(text) or ""
    synonyms = disaster_synonyms_validation.get(predicted_category, [])
    # Inclui a pr√≥pria categoria como sin√¥nimo v√°lido
    synonyms = synonyms + [predicted_category]
    # Exige pelo menos uma palavra-chave relevante no texto
    has_relevant_term = any(synonym in text for synonym in synonyms)
    if not has_relevant_term:
        return False, f"categoria {predicted_category} n√£o corresponde aos sin√¥nimos ou √† pr√≥pria categoria"
    # Verifica coer√™ncia sem√¢ntica: exige um m√≠nimo de contexto descritivo
    words = text.split()
    if len(words) < 5 or len(set(words)) < 3:
        return False, "texto sem contexto suficiente para a categoria"
    return True, "v√°lido"

def validate_impact(text, impact_level):
    text = normalize_text(text) or ""
    impact_details = impact_data.get(impact_level, impact_data["indefinido"])
    synonyms = impact_details["sinonimos_intensidade"]
    return any(synonym in text for synonym in synonyms), f"impacto {impact_level} {'v√°lido' if any(synonym in text for synonym in synonyms) else 'n√£o corresponde ao texto'}"

def validate_geographic_proximity(lat, lon, predicted_city, max_distance_km=100):
    if lat is None or lon is None:
        return False, "coordenadas ausentes"
    city_data = next((loc for loc in locations_data if loc["cidade"] == predicted_city), None)
    if not city_data or city_data["latitude"] is None or city_data["longitude"] is None:
        return False, f"cidade {predicted_city} n√£o encontrada ou sem coordenadas"
    distance = geodesic((lat, lon), (city_data["latitude"], city_data["longitude"])).km
    return distance <= max_distance_km, f"dist√¢ncia {'v√°lida' if distance <= max_distance_km else f'excede {max_distance_km} km'}"

def validate_model_confidence(model, text, predicted_label, min_confidence=0.7):
    if hasattr(model, 'predict_proba'):
        try:
            proba = model.predict_proba([text])[0]
            label_idx = model.classes_.tolist().index(predicted_label)
            confidence = proba[label_idx]
            return confidence >= min_confidence, f"confian√ßa {confidence:.2f} {'v√°lida' if confidence >= min_confidence else 'abaixo do m√≠nimo'}"
        except Exception as e:
            print(f"‚ö†Ô∏è Erro ao verificar confian√ßa do modelo: {e}")
            return False, f"erro na confian√ßa do modelo: {e}"
    return True, "confian√ßa n√£o verificada (modelo sem predict_proba)"

def validate_area_consistency(area_km2, impact_level):
    details = impact_data.get(impact_level, impact_data["indefinido"])
    return details["area_km2_min"] <= area_km2 <= details["area_km2_max"], f"√°rea {area_km2} {'v√°lida' if details['area_km2_min'] <= area_km2 <= details['area_km2_max'] else 'inconsistente com impacto'}"

def validate_regional_context(category, real_cidade):
    if real_cidade:
        city_data = next((loc for loc in locations_data if normalize_text(loc["cidade"]) == normalize_text(real_cidade)), None)
        if city_data and category in implausible_categories:
            return city_data["pais"] not in implausible_categories[category], f"categoria {category} {'v√°lida' if city_data['pais'] not in implausible_categories[category] else 'improv√°vel para o pa√≠s'}"
    return True, "contexto regional v√°lido"

# === Endpoints da API ===
@app.route("/predict", methods=["POST"])
def predict():
    data = request.get_json()
    if not data:
        return jsonify({"erro": "Nenhum dado fornecido"}), 400
    titulo = data.get("titulo", "")
    conteudo = data.get("conteudo", "")
    lat = data.get("lat")
    lon = data.get("lon")
    area_demarcada = data.get("areaDemarcada", "N/A")
    if not (titulo.strip() or conteudo.strip()):
        return jsonify({"erro": "Pelo menos t√≠tulo ou conte√∫do deve ser fornecido"}), 400
    text_input = f"{titulo.strip()} {conteudo.strip()}".strip()
    try:
        pred_cat = modelo_categoria.predict([text_input])[0] if modelo_categoria else "Modelo de Categoria n√£o carregado"
        pred_loc = modelo_localizacao.predict([text_input])[0] if modelo_localizacao else "Modelo de Localiza√ß√£o n√£o carregado"
        pred_imp = modelo_impacto.predict([text_input])[0] if modelo_impacto else "Modelo de Impacto n√£o carregado"
        cidade = pred_loc
        if lat is not None and lon is not None:
            closest_city = find_closest_city(lat, lon, max_distance_km=100)
            if closest_city:
                cidade = closest_city
        impact_level = determine_impact_level_with_area(pred_imp, area_demarcada if area_demarcada != "N/A" else 0)
        return jsonify({
            "categoria": pred_cat,
            "cidade": cidade,
            "impacto": impact_level
        })
    except Exception as e:
        print(f"Erro no endpoint /predict: {str(e)}")
        return jsonify({"erro": str(e)}), 500

@app.route('/publicar', methods=['POST'])
def receive_post():
    data = request.get_json()
    if not data or not (data.get('titulo') or data.get('conteudo')):
        return jsonify({"status": "error", "message": "T√≠tulo ou conte√∫do deve ser fornecido"}), 400
    titulo = data.get('titulo', '')
    conteudo = data.get('conteudo', '')
    lat = data.get('lat')
    lon = data.get('lon')
    marcacao = data.get('marcacao')
    message_hash = hashlib.sha256(f"{titulo}{conteudo}".encode('utf-8')).hexdigest()
    if message_hash in loaded_posts_set:
        print(f"‚è© Publica√ß√£o duplicada recebida (hash: {message_hash[:8]}...). Ignorando.")
        return jsonify({"status": "ignored", "message": "Publica√ß√£o j√° existe."}), 200
    real_cidade = None
    if lat is not None and lon is not None:
        real_cidade = get_city_from_coordinates(lat, lon)
    new_post_entry = {
        "id_hash": message_hash,
        "titulo": titulo,
        "conteudo": conteudo,
        "lat": lat,
        "lon": lon,
        "marcacao": marcacao,
        "REALcidade": real_cidade,
        "timestamp": datetime.datetime.now().isoformat()
    }
    loaded_posts_list.append(new_post_entry)
    loaded_posts_set.add(message_hash)
    save_real_posts()
    print(f"‚ú® Nova publica√ß√£o recebida e salva (hash: {message_hash[:8]}..., REALcidade: {real_cidade}). Total: {len(loaded_posts_list)}.")
    return jsonify({"status": "success", "message": "Publica√ß√£o recebida e armazenada.", "REALcidade": real_cidade}), 201

@app.route('/publicacoes', methods=['GET'])
def get_posts():
    try:
        posts = [
            {
                "id": post.get("id_hash"),
                "titulo": post.get("titulo", ""),
                "conteudo": post.get("conteudo", ""),
                "lat": post.get("lat"),
                "lon": post.get("lon"),
                "endereco": "",
                "marcacao": post.get("marcacao"),
                "REALcidade": post.get("REALcidade")
            } for post in loaded_posts_list
        ]
        return jsonify(posts), 200
    except Exception as e:
        print(f"Erro no endpoint /publicacoes: {str(e)}")
        return jsonify({"erro": str(e)}), 500

@app.route('/gerar_dataset_real', methods=['POST'])
def generate_real_dataset():
    global loaded_posts_list
    if not loaded_posts_list:
        return jsonify({"status": "error", "message": "Nenhuma publica√ß√£o encontrada em real_posts.json."}), 400
    if not (modelo_categoria and modelo_localizacao and modelo_impacto):
        return jsonify({"status": "error", "message": "Um ou mais modelos n√£o foram carregados."}), 500
    print(f"\nIniciando gera√ß√£o do dataset a partir de {len(loaded_posts_list)} publica√ß√µes de real_posts.json...")
    try:
        df_existing_real = pd.read_csv(DATASET_REAL_COMBINED_PATH) if os.path.exists(DATASET_REAL_COMBINED_PATH) else pd.DataFrame(columns=[
            "titulo", "conteudo", "categoria", "sinonimo_disparador", "idioma",
            "continente", "pais", "estado", "cidade", "latitude", "longitude",
            "impacto_nivel", "impacto_cor", "impacto_area_km2", "impacto_sinonimo_intensidade", "REALcidade"
        ])
        print(f"Dataset de publica√ß√µes reais existente carregado com {len(df_existing_real)} registros.")
    except Exception as e:
        print(f"Erro ao carregar dataset existente: {e}")
        df_existing_real = pd.DataFrame(columns=[
            "titulo", "conteudo", "categoria", "sinonimo_disparador", "idioma",
            "continente", "pais", "estado", "cidade", "latitude", "longitude",
            "impacto_nivel", "impacto_cor", "impacto_area_km2", "impacto_sinonimo_intensidade", "REALcidade"
        ])
    processed_hashes = set((df_existing_real['titulo'] + df_existing_real['conteudo']).map(lambda x: hashlib.sha256(x.encode('utf-8')).hexdigest()))
    new_data_for_df = []
    processed_count = 0
    for post in loaded_posts_list:
        titulo = post.get('titulo', '')
        conteudo = post.get('conteudo', '')
        lat = post.get('lat')
        lon = post.get('lon')
        marcacao = post.get('marcacao')
        real_cidade = post.get('REALcidade')
        timestamp = post.get('timestamp')
        message_hash = hashlib.sha256(f"{titulo}{conteudo}".encode('utf-8')).hexdigest()
        if message_hash in processed_hashes:
            print(f"‚è© Publica√ß√£o duplicada (hash: {message_hash[:8]}...). Ignorando.")
            continue
        text_input = f"{titulo} {conteudo}".strip()
        if not text_input:
            print(f"‚ö†Ô∏è Publica√ß√£o ignorada: texto vazio.")
            continue
        # Valida√ß√£o de qualidade do texto
        is_valid_text, text_reason = validate_text_quality(text_input)
        if not is_valid_text:
            print(f"‚ö†Ô∏è Publica√ß√£o '{text_input[:50]}...' ignorada: {text_reason}.")
            continue
        # Valida√ß√£o de timestamp
        is_valid_timestamp, timestamp_reason = validate_timestamp(timestamp)
        if not is_valid_timestamp:
            print(f"‚ö†Ô∏è Publica√ß√£o '{text_input[:50]}...' ignorada: {timestamp_reason}.")
            continue
        # Infer√™ncia dos modelos
        try:
            predicted_category = modelo_categoria.predict([text_input])[0]
            predicted_impact_level = modelo_impacto.predict([text_input])[0]
            predicted_city = modelo_localizacao.predict([text_input])[0]
        except Exception as e:
            print(f"‚ùå Erro na infer√™ncia para a mensagem '{text_input[:50]}...': {e}")
            continue
        # Valida√ß√£o da categoria
        is_valid_category, category_reason = validate_category(text_input, predicted_category)
        if not is_valid_category:
            print(f"‚ö†Ô∏è Publica√ß√£o '{text_input[:50]}...' ignorada: {category_reason}.")
            continue
        # Valida√ß√£o da confian√ßa do modelo
        is_valid_confidence, confidence_reason = validate_model_confidence(modelo_categoria, text_input, predicted_category)
        if not is_valid_confidence:
            print(f"‚ö†Ô∏è Publica√ß√£o '{text_input[:50]}...' ignorada: {confidence_reason}.")
            continue
        # C√°lculo da √°rea
        predicted_area = 0
        try:
            if marcacao:
                marcacao_geojson = json.loads(marcacao)
                if marcacao_geojson.get('type') == 'Feature' and marcacao_geojson.get('geometry', {}).get('type') == 'Polygon':
                    predicted_area = area(Feature(geometry=marcacao_geojson['geometry'])) / 1_000_000
                    predicted_area = round(predicted_area, 2)
        except Exception as e:
            print(f"Erro ao calcular √°rea para marcacao: {e}")
        impact_level = determine_impact_level_with_area(predicted_impact_level, predicted_area)
        # Valida√ß√£o do impacto
        is_valid_impact, impact_reason = validate_impact(text_input, impact_level)
        if not is_valid_impact:
            print(f"‚ö†Ô∏è Publica√ß√£o '{text_input[:50]}...' ignorada: {impact_reason}.")
            continue
        # Valida√ß√£o de consist√™ncia de cidade
        if real_cidade and predicted_city:
            normalized_real_cidade = normalize_text(real_cidade)
            normalized_predicted_city = normalize_text(predicted_city)
            if normalized_real_cidade != normalized_predicted_city:
                print(f"‚ö†Ô∏è Publica√ß√£o '{text_input[:50]}...' ignorada: REALcidade ({real_cidade}) != cidade predita ({predicted_city})")
                continue
        else:
            print(f"‚ö†Ô∏è Publica√ß√£o '{text_input[:50]}...' ignorada: REALcidade ou cidade predita ausente.")
            continue
        # Valida√ß√£o de proximidade geogr√°fica
        is_valid_geo, geo_reason = validate_geographic_proximity(lat, lon, predicted_city)
        if not is_valid_geo:
            print(f"‚ö†Ô∏è Publica√ß√£o '{text_input[:50]}...' ignorada: {geo_reason}.")
            continue
        # Valida√ß√£o de consist√™ncia de √°rea
        if marcacao:
            is_valid_area, area_reason = validate_area_consistency(predicted_area, impact_level)
            if not is_valid_area:
                print(f"‚ö†Ô∏è Publica√ß√£o '{text_input[:50]}...' ignorada: {area_reason}.")
                continue
        # Valida√ß√£o de contexto regional
        is_valid_context, context_reason = validate_regional_context(predicted_category, real_cidade)
        if not is_valid_context:
            print(f"‚ö†Ô∏è Publica√ß√£o '{text_input[:50]}...' ignorada: {context_reason}.")
            continue
        processed_hashes.add(message_hash)
        processed_count += 1
        predicted_lat = round(float(lat), 4) if lat is not None else None
        predicted_lon = round(float(lon), 4) if lon is not None else None
        predicted_state = "Desconhecido"
        if real_cidade:
            predicted_city = real_cidade
            found_city_data = next((loc for loc in locations_data if loc["cidade"] == predicted_city), None)
            if found_city_data:
                predicted_state = found_city_data["estado"]
                if predicted_lat is None:
                    predicted_lat = found_city_data["latitude"]
                if predicted_lon is None:
                    predicted_lon = found_city_data["longitude"]
        elif lat is not None and lon is not None:
            closest_city = find_closest_city(lat, lon, max_distance_km=100)
            if closest_city:
                predicted_city = closest_city
                found_city_data = next((loc for loc in locations_data if loc["cidade"] == predicted_city), None)
                if found_city_data:
                    predicted_state = found_city_data["estado"]
                    if predicted_lat is None:
                        predicted_lat = found_city_data["latitude"]
                    if predicted_lon is None:
                        predicted_lon = found_city_data["longitude"]
        impact_details = impact_data.get(impact_level, impact_data["indefinido"])
        predicted_impact_intensity = random.choice(impact_details["sinonimos_intensidade"])
        new_data_for_df.append({
            "titulo": titulo,
            "conteudo": conteudo,
            "categoria": predicted_category,
            "sinonimo_disparador": "publicacao_real_inferida",
            "idioma": "pt",
            "continente": "Am√©rica do Sul",
            "pais": "Brasil",
            "estado": predicted_state,
            "cidade": predicted_city,
            "latitude": predicted_lat,
            "longitude": predicted_lon,
            "impacto_nivel": impact_level,
            "impacto_cor": impact_details["cor"],
            "impacto_area_km2": predicted_area,
            "impacto_sinonimo_intensidade": predicted_impact_intensity,
            "REALcidade": real_cidade
        })
    df_new_posts = pd.DataFrame(new_data_for_df)
    df_combined_final = pd.concat([df_existing_real, df_new_posts], ignore_index=True)
    df_combined_final = df_combined_final.drop_duplicates(subset=['titulo', 'conteudo']).reset_index(drop=True)
    df_combined_final.to_csv(DATASET_REAL_COMBINED_PATH, index=False, encoding='utf-8')
    print(f"üìä Dataset de publica√ß√µes reais salvo em '{DATASET_REAL_COMBINED_PATH}'. Total de registros: {len(df_combined_final)}.")
    return jsonify({
        "status": "success",
        "message": f"Dataset de publica√ß√µes reais atualizado com {processed_count} novas publica√ß√µes inferidas.",
        "total_processed": processed_count,
        "total_received": len(loaded_posts_list)
    }), 200

@app.route("/", methods=["GET"])
def health_check():
    return jsonify({"status": "API Python funcionando!", "timestamp": datetime.datetime.now().isoformat()})

@app.route('/api/dataset_real', methods=['GET'])
def get_real_dataset():
    try:
        if not os.path.exists(DATASET_REAL_COMBINED_PATH):
            return jsonify({"status": "error", "message": "Dataset n√£o encontrado."}), 404
        with open(DATASET_REAL_COMBINED_PATH, 'r', encoding='utf-8') as f:
            return f.read(), 200, {'Content-Type': 'text/csv; charset=utf-8'}
    except Exception as e:
        print(f"Erro no endpoint /api/dataset_real: {str(e)}")
        return jsonify({"erro": str(e)}), 500

if __name__ == "__main__":
    app.run(host='0.0.0.0', port=5000, debug=True)
